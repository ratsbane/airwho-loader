#!/usr/bin/perl

use DBI;
use JSON::Parse 'json_file_to_perl';

use lib '.';
use FileSet;


$v=1;  # verbose


# Fields (columns) present in the MASTER or MASTER.txt files
# [ field name, sql data type ]
$master_fields = [ [ 'n_number', 'varchar(5)' ],
	           [ 'serial_number', 'varchar(30)'],
                   [ 'mfr_mdl_code', 'varchar(7)'],
                   [ 'eng_mfr_mdl', 'varchar(5)'],
                   [ 'year_mfr', 'varchar(4)'],
                   [ 'type_registrant', 'varchar(1)'],
                   [ 'name', 'varchar(50)'],
                   [ 'street', 'varchar(33)'],
                   [ 'street2', 'varchar(33)'],
                   [ 'city', 'varchar(18)'],
                   [ 'state', 'varchar(2)'],
                   [ 'zip_code', 'varchar(10)'],
                   [ 'region', 'varchar(1)'],
                   [ 'county', 'varchar(3)'],
                   [ 'country', 'varchar(2)'],
                   [ 'last_action_date', 'date'],    # YYYYMMDD
                   [ 'cert_issue_date', 'date'],     # YYYYMMDD 
                   [ 'certification', 'varchar(10)'],  # this is the big chunk of data that contains other fields
                   [ 'type_aircraft', 'varchar(1)'], 
                   [ 'type_engine', 'varchar(2)'], 
                   [ 'status_code', 'varchar(2)'],
                   [ 'mode_s_code', 'varchar(8)'],
                   [ 'fract_owner', 'varchar(1)'],
                   [ 'air_worth_date', 'date'],      # YYYYMMDD
                   [ 'other_names_1', 'varchar(50)'],
                   [ 'other_names_2', 'varchar(50)'],
                   [ 'other_names_3', 'varchar(50)'],
                   [ 'other_names_4', 'varchar(50)'],
                   [ 'other_names_5', 'varchar(50)'],
                   [ 'expiration_date', 'date'],      # YYYYMMDD  This field missing from the earliest files
                   [ 'unique_id', 'varchar(8)'],
                   [ 'kit_mfr', 'varchar(30)'],
                   [ 'kit_model', 'varchar(20)'],
                   [ 'mode_s_code_hex', 'varchar(10)']
                 ];
my @date_fields_in_master = map {$_->[0]} grep {$_->[1] eq 'date'} @{$master_fields};

my $config = json_file_to_perl ('config.json');
my $mysql = DBI->connect('DBI:mysql:airwho;host=localhost', $config->{'username'}, $config->{'password'});

# Load all of the files in the import_files directory structure starting with $max_date.
# At this point we don't have record that the previous run was completed successfully, so we have to consider that the
# changes stored for $max_date might not be complete.  We want inserting a/d/c records into master_changes to be idempotent
# so it's safe to redo some or all of the changes for $max_date.  For efficiency, consider adding a table to record which 
# files have been successfully imported.
($max_date) = @{$mysql->selectcol_arrayref('select max(date_from) from master_changes')};
# instantiate the files list as all available import files on or after $max_date.  If max_date is undef, start with the first file.
my $fileset = FileSet->new($max_date);
print "\$max_date is $max_date\n";
#die "all files: ".join("\n", @{$files->{'all'}})."\n";

my ($o, $o_date) = $fileset->get_one_before($maxdate); # If $maxdate represents the first one then get_one_before will
                                                       # return an empty array, which means that everything in $n will be added, which is what we want.
print "\$o_date is $o_date\n";
while (($n, $n_date) = $fileset->get_next()) {
  print "comparing with new file, $n_date\n";
  #compare_two_datasets($o, $o_date, $n, $n_date);
  $o = $n;
  $o_date = $n_date;
  }



sub compare_two_datasets {
  my @old = @{+shift};
  my $old_date = shift;
  my @new = @{+shift};
  my $new_date = shift;

  if ($v) {print "Comparing $old_date with $new_date\n";}

  # Convert $new and $old into hashes
  # The serial number is in position 1 and mfg/model code is in position 2.  Use that as the hash key.
  # The "unique_id" field might be better but that field is not present in the earlier files; it was added later.
  %new = map { ($_->[2].'|'.$_->[1]) => [ $_->[0], @{$_}[3..$#{$new[0]}] ] } @new;
  %old = map { ($_->[2].'|'.$_->[1]) => [ $_->[0], @{$_}[3..$#{$old[0]}] ] } @old;

  # Using a prepared statement is much faster than not (but still the biggest bottlneck)
  my @fields = map {$_->[0]} @{$master_fields};
  my $insert_into_master_stmt = $mysql->prepare('insert into master_changes ( date_from, date_to, change_type,'.join(', ', @fields).') values (?,?,?'. (',?'x(scalar @fields) ).')');

  my $field_count = scalar @{$master_fields};

  # Iterate through $new, looking for anything not in $old - that will give us added records.
  # Get the changed records also on this pass because we already have the new record (which is what we want to store)
  foreach $n(@new) {
    my $key = $n->[2].'|'.$n->[1];
    if ( ! $old{$key}) {
      if ($v) {print "Added: $key\n";}
      $insert_into_master_stmt->execute( $old_date, $new_date, 'A', ( map {$master_fields->[$_]->[1] eq 'date' ? format_date($n->[$_]) : $n->[$_] } 0..($field_count-1) ) );
      }
    elsif ( ! (@{$new{$key}} ~~ @{$old{$key}})) {
      if ($v) {print "Changed: $key\n";}
      $insert_into_master_stmt->execute( $old_date, $new_date, 'C', ( map {$master_fields->[$_]->[1] eq 'date' ? format_date($n->[$_]) : $n->[$_] } 0..($field_count-1) ) );
      }
    }
  # Iterate through $old, looking for anything not in $new - that will give us deleted records
  # the compare function is tricky, because at several times through the history of these files, fields have been added.
  # fortunately, all of the added fields have been added to the end of the previous set of fields.
  # NB originally we did NOT want to consider as changed a file that just has fields added.  Now we do.
  foreach $o(@old) {
    my $key = $o->[2].'|'.$o->[1];
    unless ($new{$key}) {
      if ($v) {print "Deleted: $key\n";}
      $insert_into_master_stmt->execute( $old_date, $new_date, 'D', ( map {$master_fields->[$_]->[1] eq 'date' ? format_date($o->[$_]) : $o->[$_] } 0..($field_count-1) ) );
      }
    }
  }

